# 데이터 획득 및 수집 Lv

2-1. Lv 1 

- 제공이 된다

  - 사내 데이터, 공공 데이터, 대학및 연구기관의 제공 데이터

  - 콘테스트 데이터(국내대회 해외대회(캐글kaggle)) 

  ​           => 상업성이 없고, 정제된 데이터다    

2-2. Lv 2

- 정제된 데이터다

- open API 사용

- http 통신을 통해서 응답 데이터를 통해 수집

  - ex) kakako ,naver, t, goggle 등등 포털이나 대기업 제공하는 open  API를 활용
    - 단, 쿼리 제한(일일 쿼리수)

- request

  - get / post
    -   get을 사용할 때는 requests.get()을 사용하고, post를 사용할때는 requests.post()를 사용

  

2-3. Lv 3

- web scraping (웹 스크래핑)

- 우리가 접근할수 잇는 모든 정보는 웹에서 접근이 가능하다라는 명제로 출발

- 보안 데이터는 불가

- 웹사이트를 긁어서 원하는 데이터를 추출하여 전처리 적제하는 방식

- request + beautifulsoup(bs4)

  

2-4. Lv 4

- crawling(크롤링)
- 정보의 출처가 웹사이트는 맞는데 사람의 손을 타야지만 데이터를 획득할수 있는 경우
- ajax를 사용하거나, 디도스 방어가 들어갔거나,등등 사람손을 거친후에야만
  접근가능한 사이트가 대상
- selenium(셀레니움) + 자동화(qt5 or 스케쥴러를 활용)



----

### 차이 

- 크롤링

  - **데이터를 수집하고 분류하는 것**

  - 일종의 스크래핑 기술,조직적, 자동화된 방법 
  - 웹상에 존재하는 컨텐츠를 수집하는 작업 
    - 필요한 데이터만 수집
      - open API 
      - selenium등 브라우저 프로그램을 조작해서 필요한 데이터를 추출 
      - html 페이지를 파싱해서 필요한 데이터만 추출  - BeautifulSoup

  

- 파싱 

  - 내가 원하는 데이터를 **특정 패턴이나 순서로 추출하여 정보를 가공**하는 것 -> 일련의 문자열을 의미있는 토큰으로 분해하고 이들로 만들어진 파스 트리를 만드는 과정을 말한다. 

-  스크레핑 

  - **웹사이트의 데이터를 수집하는 모든 작업 **

---

참조 

- [잔재미 코딩](https://www.fun-coding.org/crawl_basic2.html)

------

### 